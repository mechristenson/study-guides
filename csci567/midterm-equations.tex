\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb}

\title{CSCI 567 Important Equations}
\author{Mark Christenson}
\date{February 2019}
\begin{document}
\maketitle

\section{Nearest Neighbor Classifier}
\paragraph{Nearest Neighbor Logic}
\paragraph{}
\(x(1) = x_{nn(x)}\),\\\\
where \(nn(x)\epsilon[N]=\{1,2,...,N\}\), i.e. the index to one of the training instances,
\paragraph{}
\(nn(x) = argmin_{n\epsilon[N]}||x-x_n||_2=argmin_{n\epsilon[N]}\sqrt{\sum_{d=1}^D(x_d-x_{nd})^2}\),\\\\
where argmin means finding the argument (in this case, n) that minimizes the function/expression and \(||.||_2\) is the L2/Euclidean distance between the two points
\paragraph{Measuring Performance}
\paragraph{}
\(D^{TEST}=\{(x_1,y_1),(x_2,y_2),...,(x_3,y_3)\}\)
\paragraph{}
\(A^{TEST}=\frac{1}{N}\sum_nI[f(x)==y_n], \epsilon^{TEST}=\frac{1}{N}\sum_nI[f(x)\neq y_n]\),\\\\
where I[.] is the indicator function: \(I[true] = 1\), and \(I[false] = 0\)
\paragraph{Hyperparameter: Distance}
\paragraph{}
Previously, we used the Euclidean (L2) Distance:
\paragraph{}
\(nn(x) = argmin_{n\epsilon[N]}||x-x_n||_2\)\\\\
There are many other alternatives that we can use, including Manhattan (L1) Distance:
\paragraph{}
\(||x-x_n||_1=\sum_{d=1}^D|x_d-x_{nd}|\)\\\\
More generally, we can use LP distance (for \(p \geq 1\)):
\paragraph{}
\(||x-x_n||_P=(\sum_{d=1}^D(x_d-x_{nd})^P)^{\frac{1}{P}}\)
\paragraph{Hyperparameter: K-Nearest Neighbor (KNN)}
\paragraph{}
1st-Nearest Neighbor: \(nn_1(x)=argmin_{n\epsilon[N]}||x-x_n||_2\)\\
2nd-Nearest Neighbor: \(nn_2(x)=argmin_{n\epsilon[N]-nn_1(x)}||x-x_n||_2\)\\
3rd-Nearest Neighbor: \(nn_3(x)=argmin_{n\epsilon[N]-nn_1(x)-nn_2(x)}||x-x_n||_2\)\\
Set of K-Nearest Neighbor: \(knn(x)=\{nn_1(x), nn_2(x),..., nn_K(x)\}\)\\\\
Note that with \(x_k=x_{nn_k(x)}\), we have:\\\\
\(||x-x(1)||_2^2\leq||x-x(2)||_2^2\leq...||x-x(K)||_2^2\)
\paragraph{Classification Rule}
\paragraph{}
Every neighbor votes, naturally xn votes for its label yn.\\
Aggregate everyone's vote on a class label, c
\paragraph{}
\(v_c=\sum_{n\epsilon knn(x)} I(y_n==c), \forall c \epsilon[C]\)\\\\
Predict with the majority
\paragraph{}
\(y=f(x)=argmax_{c\epsilon[C]}v_c\)
\paragraph{Hyperparameter: Preprocessing Data}
\paragraph{}
One method of normalizing our data is as follows:\\
Compute the means and standard deviations in each feature
\paragraph{}
\(\hat{x_d} = \frac{1}{N}\sum_nx_{nd}, s_d^2=\frac{1}{N-1}\sum_n(x_{nd}-\hat{x_d})^2\)\\\\
Scale the feature accordingly
\paragraph{}
\(x_{nd} = \frac{x_{nd}-\hat{x_d}}{s_d}\)

\section{Decision Tree}
\paragraph{Entropy}
\paragraph{}
At each level of a tree, we can greedily choose the most optimal attribute to branch on by leveraging the Entropy function. Entropy is the non-negative measure of impurity in a system. Thus, at each node, we want to minimize entropy.
\paragraph{}
\(H(P) = -\sum_{k=1}^C P(Y=K)\log P(Y=k)\)\\\\
Note this equation is maximized if P is uniform, (H(P)=logC): most uncertain case
\paragraph{}
\(H(P) = -\sum_{k=1}^C \frac{1}{C}\log \frac{1}{C}=C*\frac{1}{C}\log C = \log C\)\\\\
Furthermore, this equation is minimized if P focuses on one class, (H(P)=0): most certain case
\paragraph{}
\(0 \log 0\) is defined naturally as \(lim_{z->0+}z\log z = 0\)\\\\
Note that we can calculate the entropy of an attribute by calculating the weighted average conditional entropy of its children
\paragraph{Information Gain}
\paragraph{}
Entropy is a measure of the purity. We can leverage this to generate a measure of effectiveness of an attribute. The information gain of a node n with children Values(A) is:
\paragraph{}
\(Gain(n, A) = Entropy(S_n) -\sum_{m\epsilon Values(A)} \frac{|S_m|}{|S_n|}Entropy(S_m)\),\\\\
where Sn and Sm are the subsets of training examples that belong to the node n and one of its child node m respectively.\\\\
Information Gain = entropy(parent) - [average entropy(children)]\\\\
Gain(n, A) is the expected reduction in entropy caused by partitioning on the values of attribute A.
\paragraph{Hyperparameter: Gini Impurity}
\paragraph{}
Some algoriuthms, C4.5, CART, etc, leverage a different type of impurity function. Rather than using entropy these algorithms use the gini impurity
\paragraph{}
\(G(P) = -\sum_{k=1}^C P(Y=k)(1-P(Y=k))\)

\section{Naive Bayes}
\paragraph{Bayes Optimal Classifier}
\paragraph{}
Suppose the data \((x_n, y_n)\) is drawn from a joint distribution, p. The Bayes Optimal Classifier is:
\paragraph{}
\(f^*(x) = argmax_{c\epsilon[C]}P(c|x)\),\\\\
i.e. we predict the class with the largest conditional probability\\\\
How hard is it to learn the optimal classifier?\\\\
Exponential, \(2^D\)
\paragraph{Bayes Rule}
\paragraph{}
\(P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\)
\paragraph{Naive Bayes Assumption}
\paragraph{}
\(x_d\) are conditionally independent given y, which means:
\paragraph{}
\(P(x|y=c)=\prod_{d=1}^D P(x_d|y=c)\),\\\\
reducing our complexity to linear
\paragraph{Naive Bayes Classification Example}
\paragraph{}
If we want to build a spam detector, we can compute for any document x,
\paragraph{}
\(p(spam|x)=\frac{p(x|spam)p(spam)}{p(x)}\),\(p(ham|x)=\frac{p(x|ham)p(ham)}{p(x)}\),\\\\
Its convenient to compute the logarithms, so we only need to compare
\paragraph{}
\(log[p(x|spam)p(spam)]\), versus \(log[p(x|ham)p(ham)]\),\\\\
as the denominators are the same. Furthermore:
\paragraph{}
\(log[p(x|spam)p(spam)] = log[\prod_{i} p(w_i|spam)^{\#w_i}p(spam)]=\\\\\sum_{i} \#w_i \log p(w_i|spam) + \log p(spam)\),\\\\
Thus, by leveraging the log p(spam) and log p (ham) "priors", we are back to the idea of comparing weighted sum of the number of word occurrences
\paragraph{Naive Bayes Formal Definition}
\paragraph{}
Given a random variable \(X \epsilon R^D\) and a dependent variable \(Y \epsilon [C]\), the Naive Bayes model defines the joint distribution
\paragraph{}
\(P(X=x, Y=y)=P(Y=y)P(X=x|Y=y)=\\\\P(Y=y)\prod_{d=1}^D P(X_d = x_d|Y=y)\)
\paragraph{How do we make predictions?}
\paragraph{}
Using the Naive Bayes Assumption:
\paragraph{}
\(P(x|y=c)=\prod_{d=1}^D P(x_d |y=c)\),\\\\
The prediction for a new example, x is
\paragraph{}
\(argmax_{c\epsilon[C]}P(y=c|x)=argmax_{c\epsilon[C]}\frac{P(x|y=c)P(y=c)}{P(X)}\)\\
\(=argmax_{c\epsilon[C]}(P(y=c) \prod_{d=1}^D P(x_d|y=c))\)\\
\(=argmax_{c\epsilon[C]}(\ln P(y=c) + \sum_{d=1}^D \ln P(x_d|y=c))\)\\
\paragraph{For Discrete Features}
\paragraph{}
For a label \(c\epsilon[C]\),
\paragraph{}
\(P(y=c)=\frac{|\{n:y_n=c\}|}{N}=\frac{number-of-data-points-labeled-as-c}{N}\)\\\\
For each possible value k of a discrete feature, d
\paragraph{}
\(P(x_d=k|y=c)=\frac{|\{n:x_{nd}=k, y_n=c\}|}{|\{n:y_n=c\}|}\)\\\\
They can be estimated separately
\paragraph{For Continuous Features}
\paragraph{}
We can do parametric estimation via a Gaussian
\paragraph{}
\(P(x_d=k|y=c)=\frac{1}{\sqrt{2\pi}\sigma_{cd}}exp(-\frac{(x-\mu_{cd})^2}{2\sigma^2_{cd}})\)\\\\
Where \(\mu_{cd}\), and \(\sigma^2_{cd}\) are the empirical mean and variance of feature d among all examples with label c.

\section{Math Notes}
\subsection{Vector Distances}
\subsubsection{$L_1\;Norm$}
Definition: The Manhattan distance of a vector from another vector.\\
Syntax: $||.||_1$\\
Formally, $||x-x_n||_1 = \sum^D_{d=1}{|x_d - x_{nd}|}$

\subsubsection{$L_2\;Norm$}
Definition: The Euclidean distance of a vector from another vector.\\
Syntax: $||.||_2$\\
Formally, $||x-x_n||_2 = \sqrt{\sum^D_{s=1}{(x_d - x_{nd})^2}}$\\
Note: $||x||_2^2 = x \cdot x $

\subsubsection{$L_P\;Norm$}
Definition: The Euclidean distance of a vector from another vector.\\
Syntax: $||.||_P$\\
Formally, $||x-x_n||_P = (\sum^D_{s=1}{(x_d - x_{nd})^P})^{1/P}$

\subsection{Indicator Function}
The indicator function $I$, denotes whether an expression is true or not\\
- If x is true: I(x) = true\\
- If x is false: I(x) = false

\subsection{Indicator Function}
The indicator function $I$, denotes whether an expression is true or not\\
- If x is true: I(x) = true\\
- If x is false: I(x) = false

\subsection{Variance}
Variance is a calculation of the deviations from the mean:\\
\begin{align*}
V(X) = \sigma^2 = \sum_D(x-\mu)^2 * p(x) = E[(X-\mu)^2]8
\end{align*}
where p(x) is the probability density function, and $\mu$ is the expected value


\subsection{Standard Deviation}
Standard Deviation is the square root of variance:\\
\begin{align*}
\sigma_x = \sqrt{\sigma_x}
\end{align*}

\subsection{Random Variable}
A random variable is a function that maps a sample space to the set of real numbers\\
The expected value of a random variable is what its average value would be if you ran the experiment millions of times. More formally the expected value of our random variable, X is:\\
\begin{align*}
E[X] = \sum{Pr(X=k) * X(k)}
\end{align*}

\subsection{Vectors}
A vector is a tuple of one or more values called scalars. Vectors are often represented using a lowercase character such as v; for example:
\begin{align*}
v = (v_1, v_2, v_3)
\end{align*}
Where $v_1, v_2, v_3$ are scalar values.

\subsubsection{Vector Addition}
Two vectors of equal length can be added together:
\begin{align*}
c = a + b
\end{align*}
This operation is performed element-wise to result in a new vector of the same length:
\begin{align*}
a + b = (a_1 + b_1, a_2 + b_2, a_3 + b_3)
\end{align*}

\subsubsection{Vector Subtraction}
Two vectors of equal length can be subtracted from one another:
\begin{align*}
c = a - b = a + (-b)
\end{align*}
This operation is performed element-wise to result in a new vector of the same length:
\begin{align*}
a + b = (a_1 - b_1, a_2 - b_2, a_3 - b_3)
\end{align*}

\subsubsection{Vector Multiplication}
Two vectors of equal length can be multiplied together:
\begin{align*}
c = a * b
\end{align*}
As with addition and subtraction, this operation is performed element-wise to result in a new vector of the same length:
\begin{align*}
a * b = (a_1 * b_1, a_2 * b_2, a_3 * b_3)
\end{align*}

\subsubsection{Vector Divison}
Vector division cannot be uniquely defined in terms of matricies, because there is no unique matrix solution A to the matrix equation:
\begin{align*}
y = Ax
\end{align*}

\subsection{Matricies}
\subsubsection{Invertible Matricies}
When perform an eigendecomposition, we see:
\begin{align*}
X^TX = U^T \left(\begin{array}{cccc}
\lambda_1 & 0 & ... & 0\\
0 & \lambda_2 & ... & 0\\
... & ... & ... & ...\\
0 & ... & \lambda_D & 0\\
0 & ... & 0 & \lambda_{D+1}
\end{array}\right) U
\end{align*}
where $\lambda_1  \geq \lambda_2  \geq ... \lambda_{D+1}  \geq 0 $\\
The inverse is:
\begin{align*}
(X^TX)^{-1} = U^T \left(\begin{array}{cccc}
\frac{1}{\lambda_1} & 0 & ... & 0\\
0 & \frac{1}{\lambda_2} & ... & 0\\
... & ... & ... & ...\\
0 & ... & \frac{1}{\lambda_D} & 0\\
0 & ... & 0 & \frac{1}{\lambda_{D+1}}
\end{array}\right) U
\end{align*}

If some eigenvalues are 0, then the matrix is non-invertible

\subsection{Taylor Approximation}
A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the fucntion's derivatives at a single point.
\subsubsection{First Order Taylor Approximation}
\begin{align*}
F(w) \approx F(w^{(t)}) + \nabla F(w^{(t)})^T(w-w^{(t)})
\end{align*}
Where $\nabla F(w)$ is the gradient, or first derivative of the vector w
\subsubsection{Second Order Taylor Approximation}
\begin{align*}
F(w) \approx F(w^{(t)}) + \nabla F(w^{(t)})^T(w-w^{(t)}) + \frac{1}{2}(w-w^{(t)})^T  H_t(w-w^{(t)})
\end{align*}
Where $H(w)$ is the hessian, or second derivative of the vector w

\subsection{Derivatives}
\subsubsection{Natural Logarithm}
\begin{align*}
D_x(ln(x)) = 1/x\\
D_x(ln(Kx)) = 1/x\\
D_x(ln(x^2)) = 2/x
\end{align*}

\subsubsection{Exponential Function}
\begin{align*}
D_x(e^x)) = e^x\\
D_x(e^{2x})) = 2e^{2x}\\
D_x(e^{x^2})) = 2xe^{x^2}
\end{align*}
\end{document}
